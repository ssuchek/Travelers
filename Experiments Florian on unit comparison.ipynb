{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Florian\\Anaconda3\\envs\\data-x\\lib\\site-packages\\requests\\__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.6) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "from genericpath import exists\n",
    "import os\n",
    "import inflect\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    nltk.find('corpora/wordnet')\n",
    "except:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "try:\n",
    "    nltk.find('corpora/stopwords')\n",
    "except:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "import config as constants\n",
    "from config import config\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils.loader.loader import ClaimDataLoader\n",
    "\n",
    "from utils.helpers import format_and_regex\n",
    "from utils.logging.helpers import log_and_warn, log_initialize\n",
    "\n",
    "log_initialize(\n",
    "    file_path=config[\"logging\"][\"log_path_claim_data\"],\n",
    "    file_mode=constants.LOG_FILE_MODE,\n",
    "    log_level=constants.LOG_LEVEL,\n",
    "    log_format_str=constants.LOG_FORMAT,\n",
    "    days_keep=7\n",
    ")\n",
    "\n",
    "from utils.preprocess import WEIGHTS_PREPROCESS, WEIGHTS_WORD_PREPROCESS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_db_file = config[\"data\"][\"weights_db\"]\n",
    "weights_db = loader.preprocess_weights_db(weights_db_file=weights_db_file, filename=config[\"data\"][\"weights_preprocessed_db\"])\n",
    "\n",
    "claim_data = pd.read_csv('data/output/revised/single_year/Pentatonic_Xact_Categories_5yr_categorized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the weights match\n",
      "1\n",
      "EA\n",
      "---\n",
      "length of the weights match\n",
      "1\n",
      "---\n",
      "length of the weights match\n",
      "5\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "---\n",
      "length of the weights match\n",
      "1\n",
      "EA\n",
      "EA\n",
      "EA\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 3\n",
    "data = claim_data.copy()\n",
    "\n",
    "item_desc = data[\"item_description\"].unique().tolist()\n",
    "category_desc = data[\"subcategory_prev\"].unique().tolist()\n",
    "item_desc = [item for item in item_desc if item == item]\n",
    "category_desc = [category for category in category_desc if category == category]\n",
    "weights = weights_db\n",
    "weight_descriptions = weights_db[[\"primary_desc\", \"secondary_desc\", \"material\", \"dimensions\", \n",
    "                                  \"values_desc\", \"unit\"]].iloc[613:617].to_dict('records')\n",
    "           \n",
    "for desc in weight_descriptions:\n",
    "    matched_mask = True\n",
    "    matched_item_desc = item_desc.copy() #all item descriptions in here from claims in the beginning\n",
    "    matched_category_desc = category_desc.copy() # all item descriptions in here from claims in the beginning\n",
    "\n",
    "    for key in desc: #\n",
    "\n",
    "        if desc[key]:\n",
    "\n",
    "            #logging.info(\"Processing {} '{}'...\".format(key, desc[key]))\n",
    "            \n",
    "            if key != 'unit':\n",
    "\n",
    "                if \";\" not in desc[key]:\n",
    "                    desc_split = [\" \".join(desc[key].split()[i:i+chunk_size]) for i in range(0, len(desc[key].split()), chunk_size)]\n",
    "\n",
    "                    if len(desc[key].split()) > 3:\n",
    "                        logging.info(\"Length of '{}' is too long for regex search. Splitting in chunks\".format(desc[key]))\n",
    "                else:\n",
    "                    desc_split = [desc[key]]\n",
    "\n",
    "                for desc_chunk in desc_split:     \n",
    "\n",
    "                    #logging.info(\"Processing chunk '{}' from {} '{}'\".format(desc_chunk, key, desc[key]))\n",
    "\n",
    "                    compiled_regex_desc = re.compile(format_and_regex(desc_chunk.lower(), permutations=True, is_synonyms=True))\n",
    "\n",
    "                    #logging.info(\"Regex search: {}\".format(compiled_regex_desc))\n",
    "\n",
    "                    matched_item_desc = list(filter(compiled_regex_desc.match, matched_item_desc))\n",
    "                    matched_category_desc = list(filter(compiled_regex_desc.match, matched_category_desc))\n",
    "\n",
    "                    #florian - test output\n",
    "                    #log_and_warn('testing the output here:{} / {}'.format(matched_item_desc, matched_category_desc))\n",
    "\n",
    "                    if len(matched_item_desc) == 0 and len(matched_category_desc) == 0:\n",
    "                        break\n",
    "\n",
    "                matched_mask &= (weights[key] == desc[key])\n",
    "            \n",
    "                #print('--')\n",
    "                #print(key)\n",
    "                #print(matched_item_desc)\n",
    "                #print('--')\n",
    "                \n",
    "            if key == 'unit':\n",
    "                cleaned_matched_items = []\n",
    "                #print('claims data:')\n",
    "                #print(matched_item_desc)\n",
    "                print('length of the weights match')\n",
    "                print(len(weights[matched_mask]))\n",
    "                unit_weights = weights[matched_mask].iloc[0]['unit']\n",
    "                for el in matched_item_desc:\n",
    "                    unit_claims = data[data.item_description == el].iloc[0]['item_unit_cd']\n",
    "                    print(unit_weights)\n",
    "                    if unit_claims == unit_weights:\n",
    "                        cleaned_matched_items.append(el)\n",
    "                #print('matched with:')\n",
    "                #print(weights[matched_mask])\n",
    "                #print(weights[matched_mask]['unit'])\n",
    "                #print('----')\n",
    "                #print(desc)\n",
    "                #print(len(cleaned_matched_items))\n",
    "                #print(len(matched_item_desc))\n",
    "            \n",
    "    #print('CHECK OUT THE MASK')\n",
    "    #print(matched_mask)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_matched_items)\n",
    "\n",
    "len(matched_item_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data[data.item_description == el].iloc[0]['item_unit_cd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SF\n",
      "SF\n",
      "SF\n",
      "SF\n",
      "SF\n",
      "SF\n",
      "SF\n",
      "SF\n",
      "SF\n",
      "SF\n",
      "SF\n",
      "SF\n",
      "SF\n",
      "SF\n"
     ]
    }
   ],
   "source": [
    "for el in matched_item_desc:\n",
    "    print(data[data.item_description == el].iloc[0]['item_unit_cd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LF'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.item_description == 'COUNTERTOP - FLAT LAID PLASTIC LAMINATE'].iloc[0]['item_unit_cd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
